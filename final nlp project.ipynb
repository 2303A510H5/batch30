{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNi1p/po3QowOFYi11xGu/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A510H5/batch30/blob/main/final%20nlp%20project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xeN521QdlFm",
        "outputId": "426f11cb-0b8a-4a1d-b32e-c1082a9ce634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8ceebb7878ad4f6f8f0aa2db3ae034400fb37b185cac341aad4ee57b7bfb828e\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2x-o5aNd6Sn",
        "outputId": "9dafafcb-cb0b-4772-fce4-c4dd79d5b3e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable mixed precision for faster training\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_excel('/content/nlp.xlsx')\n",
        "\n",
        "en_col = df.columns[0]\n",
        "te_col = df.columns[1]\n",
        "\n",
        "english_texts = df[en_col].astype(str).tolist()\n",
        "telugu_texts = df[te_col].astype(str).tolist()\n",
        "\n",
        "print(f\"Dataset size: {len(english_texts)} pairs\")\n",
        "\n",
        "# Advanced preprocessing with data cleaning\n",
        "def advanced_preprocess(text, is_telugu=False):\n",
        "    text = str(text).strip()\n",
        "    if not is_telugu:\n",
        "        text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Keep only valid characters\n",
        "    if is_telugu:\n",
        "        text = re.sub(r'[^\\u0C00-\\u0C7F\\s.,!?]', '', text)\n",
        "    else:\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# DATA AUGMENTATION - Critical for high scores\n",
        "def augment_data(english_texts, telugu_texts):\n",
        "    augmented_en = []\n",
        "    augmented_te = []\n",
        "\n",
        "    for en, te in zip(english_texts, telugu_texts):\n",
        "        # Original\n",
        "        augmented_en.append(en)\n",
        "        augmented_te.append(te)\n",
        "\n",
        "        # Add variations with punctuation\n",
        "        if not en.endswith('.'):\n",
        "            augmented_en.append(en + '.')\n",
        "            augmented_te.append(te)\n",
        "\n",
        "        # Word order variations for short sentences\n",
        "        words = en.split()\n",
        "        if 3 <= len(words) <= 7:\n",
        "            # Shuffle for simple sentences\n",
        "            np.random.seed(42)\n",
        "            if np.random.random() > 0.7:\n",
        "                shuffled = ' '.join(np.random.permutation(words[:3]).tolist() + words[3:])\n",
        "                augmented_en.append(shuffled)\n",
        "                augmented_te.append(te)\n",
        "\n",
        "    return augmented_en, augmented_te\n",
        "\n",
        "print(\"\\nPreprocessing and augmenting data...\")\n",
        "english_texts = [advanced_preprocess(text, False) for text in english_texts]\n",
        "telugu_texts = [advanced_preprocess(text, True) for text in telugu_texts]\n",
        "\n",
        "# Remove empty or very short pairs\n",
        "valid_pairs = [(e, t) for e, t in zip(english_texts, telugu_texts)\n",
        "               if len(e.split()) >= 2 and len(t.split()) >= 2]\n",
        "english_texts, telugu_texts = zip(*valid_pairs)\n",
        "\n",
        "# Augment data\n",
        "english_texts, telugu_texts = augment_data(list(english_texts), list(telugu_texts))\n",
        "\n",
        "print(f\"After augmentation: {len(english_texts)} pairs\")\n",
        "\n",
        "# Add tokens\n",
        "telugu_texts = ['<start> ' + text + ' <end>' for text in telugu_texts]\n",
        "\n",
        "# Advanced tokenization with larger vocabulary\n",
        "print(\"Creating advanced tokenizers...\")\n",
        "eng_tokenizer = keras.preprocessing.text.Tokenizer(\n",
        "    filters='',\n",
        "    oov_token='<OOV>',\n",
        "    num_words=15000  # Increased vocabulary\n",
        ")\n",
        "tel_tokenizer = keras.preprocessing.text.Tokenizer(\n",
        "    filters='',\n",
        "    oov_token='<OOV>',\n",
        "    num_words=15000\n",
        ")\n",
        "\n",
        "eng_tokenizer.fit_on_texts(english_texts)\n",
        "tel_tokenizer.fit_on_texts(telugu_texts)\n",
        "\n",
        "eng_vocab_size = min(len(eng_tokenizer.word_index) + 1, 15000)\n",
        "tel_vocab_size = min(len(tel_tokenizer.word_index) + 1, 15000)\n",
        "\n",
        "print(f\"English vocabulary size: {eng_vocab_size}\")\n",
        "print(f\"Telugu vocabulary size: {tel_vocab_size}\")\n",
        "\n",
        "# Convert to sequences\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(english_texts)\n",
        "tel_sequences = tel_tokenizer.texts_to_sequences(telugu_texts)\n",
        "\n",
        "# Dynamic padding\n",
        "max_eng_len = min(50, max(len(seq) for seq in eng_sequences))\n",
        "max_tel_len = min(50, max(len(seq) for seq in tel_sequences))\n",
        "\n",
        "print(f\"Max English length: {max_eng_len}\")\n",
        "print(f\"Max Telugu length: {max_tel_len}\")\n",
        "\n",
        "eng_padded = keras.preprocessing.sequence.pad_sequences(\n",
        "    eng_sequences, maxlen=max_eng_len, padding='post'\n",
        ")\n",
        "tel_padded = keras.preprocessing.sequence.pad_sequences(\n",
        "    tel_sequences, maxlen=max_tel_len, padding='post'\n",
        ")\n",
        "\n",
        "# Split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    eng_padded, tel_padded, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# ADVANCED TRANSFORMER-BASED MODEL\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Cast positional encoding to the input's dtype\n",
        "        return inputs + tf.cast(self.pos_encoding[:, :tf.shape(inputs)[1], :], inputs.dtype)\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.dropout3 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        attn1 = self.att1(inputs, inputs)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + inputs)\n",
        "\n",
        "        attn2 = self.att2(out1, enc_output)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(ffn_output + out2)\n",
        "\n",
        "# Build Advanced Transformer Model\n",
        "embed_dim = 512  # Increased\n",
        "num_heads = 8    # Multi-head attention\n",
        "ff_dim = 2048    # Feed-forward dimension\n",
        "num_layers = 6   # Stacked layers\n",
        "\n",
        "print(\"\\nBuilding Advanced Transformer Model...\")\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(eng_vocab_size, embed_dim)(encoder_inputs)\n",
        "x = PositionalEncoding(max_eng_len, embed_dim)(x)\n",
        "\n",
        "for _ in range(num_layers):\n",
        "    x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x, training=True)\n",
        "\n",
        "encoder_outputs = x\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "y = layers.Embedding(tel_vocab_size, embed_dim)(decoder_inputs)\n",
        "y = PositionalEncoding(max_tel_len, embed_dim)(y)\n",
        "\n",
        "for _ in range(num_layers):\n",
        "    y = TransformerDecoder(embed_dim, num_heads, ff_dim)(y, encoder_outputs, training=True)\n",
        "\n",
        "decoder_outputs = layers.Dense(tel_vocab_size, activation=\"softmax\", dtype='float32')(y)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Training configuration WITHOUT accuracy metric\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "transformer.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy'\n",
        "    # No metrics specified - only loss will be tracked\n",
        ")\n",
        "\n",
        "print(transformer.summary())\n",
        "\n",
        "# Prepare training data\n",
        "decoder_input_data = y_train[:, :-1]\n",
        "decoder_target_data = y_train[:, 1:]\n",
        "\n",
        "# Callbacks for better training\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
        "    keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "# Training\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING ADVANCED TRANSFORMER MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = transformer.fit(\n",
        "    [X_train, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=32,  # Smaller batch for better convergence\n",
        "    epochs=100,     # More epochs with early stopping\n",
        "    validation_split=0.1,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Load best model\n",
        "transformer = keras.models.load_model('best_model.keras', custom_objects={\n",
        "    'PositionalEncoding': PositionalEncoding,\n",
        "    'TransformerEncoder': TransformerEncoder,\n",
        "    'TransformerDecoder': TransformerDecoder\n",
        "})\n",
        "\n",
        "# Advanced prediction with beam search\n",
        "def beam_search_decode(input_seq, beam_width=5):\n",
        "    enc_out = transformer.layers[2](transformer.layers[1](input_seq))\n",
        "\n",
        "    sequences = [[[], 0.0]]\n",
        "\n",
        "    for _ in range(max_tel_len):\n",
        "        all_candidates = []\n",
        "\n",
        "        for seq, score in sequences:\n",
        "            if len(seq) > 0 and seq[-1] == tel_tokenizer.word_index.get('<end>', 0):\n",
        "                all_candidates.append([seq, score])\n",
        "                continue\n",
        "\n",
        "            if len(seq) == 0:\n",
        "                dec_input = np.array([[tel_tokenizer.word_index['<start>']]])\n",
        "            else:\n",
        "                dec_input = np.array([seq])\n",
        "\n",
        "            dec_input = keras.preprocessing.sequence.pad_sequences(\n",
        "                [dec_input[0]], maxlen=max_tel_len, padding='post'\n",
        "            )\n",
        "\n",
        "            predictions = transformer.predict([input_seq, dec_input], verbose=0)\n",
        "\n",
        "            # Get top k predictions\n",
        "            top_k = np.argsort(predictions[0, len(seq)])[-beam_width:]\n",
        "\n",
        "            for k in top_k:\n",
        "                candidate = [seq + [k], score - np.log(predictions[0, len(seq), k] + 1e-10)]\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda x: x[1])\n",
        "        sequences = ordered[:beam_width]\n",
        "\n",
        "    return sequences[0][0]\n",
        "\n",
        "def translate(sentence):\n",
        "    sentence = advanced_preprocess(sentence, False)\n",
        "    inputs = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    inputs = keras.preprocessing.sequence.pad_sequences(\n",
        "        inputs, maxlen=max_eng_len, padding='post'\n",
        "    )\n",
        "\n",
        "    # Use beam search for better results\n",
        "    output = beam_search_decode(inputs, beam_width=5)\n",
        "\n",
        "    result = []\n",
        "    for idx in output:\n",
        "        word = tel_tokenizer.index_word.get(idx, '')\n",
        "        if word and word not in ['<start>', '<end>', '<OOV>']:\n",
        "            result.append(word)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# EVALUATION - BLEU AND ROUGE ONLY\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION: BLEU & ROUGE SCORES ONLY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions = []\n",
        "references = []\n",
        "english_samples = []\n",
        "\n",
        "num_eval = min(500, len(X_test))  # Evaluate on more samples\n",
        "\n",
        "for i in range(num_eval):\n",
        "    eng_text = ' '.join([\n",
        "        eng_tokenizer.index_word.get(idx, '')\n",
        "        for idx in X_test[i] if idx != 0\n",
        "    ])\n",
        "\n",
        "    tel_text = ' '.join([\n",
        "        tel_tokenizer.index_word.get(idx, '')\n",
        "        for idx in y_test[i]\n",
        "        if idx != 0 and tel_tokenizer.index_word.get(idx) not in ['<start>', '<end>', '<OOV>']\n",
        "    ])\n",
        "\n",
        "    pred = translate(eng_text)\n",
        "\n",
        "    if pred.strip() and tel_text.strip():\n",
        "        predictions.append(pred)\n",
        "        references.append(tel_text)\n",
        "        english_samples.append(eng_text)\n",
        "\n",
        "    if i < 10:\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"English: {eng_text}\")\n",
        "        print(f\"Reference: {tel_text}\")\n",
        "        print(f\"Predicted: {pred}\")\n",
        "\n",
        "# Calculate BLEU Score with multiple methods\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BLEU SCORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "smooth = SmoothingFunction()\n",
        "bleu_scores = {'bleu1': [], 'bleu2': [], 'bleu3': [], 'bleu4': []}\n",
        "\n",
        "for pred, ref in zip(predictions, references):\n",
        "    pred_tokens = pred.split()\n",
        "    ref_tokens = [ref.split()]\n",
        "\n",
        "    try:\n",
        "        bleu_scores['bleu1'].append(\n",
        "            sentence_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0),\n",
        "                         smoothing_function=smooth.method4)\n",
        "        )\n",
        "        bleu_scores['bleu2'].append(\n",
        "            sentence_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0),\n",
        "                         smoothing_function=smooth.method4)\n",
        "        )\n",
        "        bleu_scores['bleu3'].append(\n",
        "            sentence_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0),\n",
        "                         smoothing_function=smooth.method4)\n",
        "        )\n",
        "        bleu_scores['bleu4'].append(\n",
        "            sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                         smoothing_function=smooth.method4)\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "avg_bleu1 = np.mean(bleu_scores['bleu1']) * 100\n",
        "avg_bleu2 = np.mean(bleu_scores['bleu2']) * 100\n",
        "avg_bleu3 = np.mean(bleu_scores['bleu3']) * 100\n",
        "avg_bleu4 = np.mean(bleu_scores['bleu4']) * 100\n",
        "\n",
        "print(f\"BLEU-1: {avg_bleu1:.2f}%\")\n",
        "print(f\"BLEU-2: {avg_bleu2:.2f}%\")\n",
        "print(f\"BLEU-3: {avg_bleu3:.2f}%\")\n",
        "print(f\"BLEU-4: {avg_bleu4:.2f}%\")\n",
        "\n",
        "# Calculate ROUGE Score\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ROUGE SCORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rouge = Rouge()\n",
        "try:\n",
        "    valid_pairs = [(p, r) for p, r in zip(predictions, references)\n",
        "                   if p.strip() and r.strip() and len(p.split()) > 0 and len(r.split()) > 0]\n",
        "\n",
        "    if valid_pairs:\n",
        "        valid_preds, valid_refs = zip(*valid_pairs)\n",
        "        rouge_scores = rouge.get_scores(list(valid_preds), list(valid_refs), avg=True)\n",
        "\n",
        "        print(f\"ROUGE-1 F1: {rouge_scores['rouge-1']['f']*100:.2f}%\")\n",
        "        print(f\"ROUGE-2 F1: {rouge_scores['rouge-2']['f']*100:.2f}%\")\n",
        "        print(f\"ROUGE-L F1: {rouge_scores['rouge-l']['f']*100:.2f}%\")\n",
        "\n",
        "        print(f\"\\nROUGE-1 Precision: {rouge_scores['rouge-1']['p']*100:.2f}%\")\n",
        "        print(f\"ROUGE-1 Recall: {rouge_scores['rouge-1']['r']*100:.2f}%\")\n",
        "        print(f\"ROUGE-2 Precision: {rouge_scores['rouge-2']['p']*100:.2f}%\")\n",
        "        print(f\"ROUGE-2 Recall: {rouge_scores['rouge-2']['r']*100:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Error calculating ROUGE: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal predictions evaluated: {len(predictions)}\")\n",
        "print(f\"Model saved as: best_model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tl-BgPSXeAKJ",
        "outputId": "d4456ce6-cb67-4936-ece8-c726803d52d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Loading dataset...\n",
            "Dataset size: 5000 pairs\n",
            "\n",
            "Preprocessing and augmenting data...\n",
            "After augmentation: 5896 pairs\n",
            "Creating advanced tokenizers...\n",
            "English vocabulary size: 9486\n",
            "Telugu vocabulary size: 10024\n",
            "Max English length: 20\n",
            "Max Telugu length: 22\n",
            "\n",
            "Training samples: 5011\n",
            "Testing samples: 885\n",
            "\n",
            "Building Advanced Transformer Model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,856,832</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encoding │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ positional_encod… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,132,288</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,503,168</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ positional_encod… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,905,600</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,142,312</span> │ transformer_deco… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10024</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m4,856,832\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encoding │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ positional_encod… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m5,132,288\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m10,503,168\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ positional_encod… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m18,905,600\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m5,142,312\u001b[0m │ transformer_deco… │\n",
              "│                     │ \u001b[38;5;34m10024\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,584,040</span> (730.84 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m191,584,040\u001b[0m (730.84 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,584,040</span> (730.84 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m191,584,040\u001b[0m (730.84 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "\n",
            "============================================================\n",
            "TRAINING ADVANCED TRANSFORMER MODEL\n",
            "============================================================\n",
            "Epoch 1/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 2s/step - loss: 3.8026 - val_loss: 2.9925 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - loss: 2.6731 - val_loss: 2.2968 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 2s/step - loss: 2.0819 - val_loss: 2.2814 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 320ms/step - loss: 1.9684 - val_loss: 2.2936 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.9440 - val_loss: 2.2907 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 314ms/step - loss: 1.8699 - val_loss: 2.3230 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.8399 - val_loss: 2.3600 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.8140 - val_loss: 2.3601 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.7925 - val_loss: 2.3445 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.7498 - val_loss: 2.3557 - learning_rate: 5.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.7495 - val_loss: 2.3552 - learning_rate: 5.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 311ms/step - loss: 1.7257 - val_loss: 2.3485 - learning_rate: 5.0000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.6779 - val_loss: 2.3318 - learning_rate: 5.0000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 312ms/step - loss: 1.6541 - val_loss: 2.3244 - learning_rate: 2.5000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - loss: 1.6494 - val_loss: 2.2739 - learning_rate: 2.5000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - loss: 1.6411 - val_loss: 2.2371 - learning_rate: 2.5000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 2s/step - loss: 1.6373 - val_loss: 2.2035 - learning_rate: 2.5000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 1.6063 - val_loss: 2.2594 - learning_rate: 2.5000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2s/step - loss: 1.5972 - val_loss: 2.1503 - learning_rate: 2.5000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 1.5549 - val_loss: 2.1671 - learning_rate: 2.5000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 318ms/step - loss: 1.5388 - val_loss: 2.1842 - learning_rate: 2.5000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 2s/step - loss: 1.5317 - val_loss: 2.1321 - learning_rate: 2.5000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 3s/step - loss: 1.5119 - val_loss: 2.0997 - learning_rate: 2.5000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 1.4925 - val_loss: 2.1028 - learning_rate: 2.5000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - loss: 1.4910 - val_loss: 2.0582 - learning_rate: 2.5000e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 1.4456 - val_loss: 2.0805 - learning_rate: 2.5000e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 2s/step - loss: 1.4343 - val_loss: 2.0463 - learning_rate: 2.5000e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 2s/step - loss: 1.4196 - val_loss: 2.0394 - learning_rate: 2.5000e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 1.3932 - val_loss: 2.0500 - learning_rate: 2.5000e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 3s/step - loss: 1.3765 - val_loss: 2.0148 - learning_rate: 2.5000e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - loss: 1.3586 - val_loss: 1.9961 - learning_rate: 2.5000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 2s/step - loss: 1.3673 - val_loss: 1.9875 - learning_rate: 2.5000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 1.3165 - val_loss: 1.9887 - learning_rate: 2.5000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 2s/step - loss: 1.3204 - val_loss: 1.9707 - learning_rate: 2.5000e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 2s/step - loss: 1.3123 - val_loss: 1.9535 - learning_rate: 2.5000e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 2s/step - loss: 1.2759 - val_loss: 1.9288 - learning_rate: 2.5000e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 2s/step - loss: 1.2563 - val_loss: 1.9193 - learning_rate: 2.5000e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 320ms/step - loss: 1.2554 - val_loss: 1.9418 - learning_rate: 2.5000e-05\n",
            "Epoch 39/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 309ms/step - loss: 1.2372 - val_loss: 1.9298 - learning_rate: 2.5000e-05\n",
            "Epoch 40/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 2s/step - loss: 1.2128 - val_loss: 1.8908 - learning_rate: 2.5000e-05\n",
            "Epoch 41/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 2s/step - loss: 1.2018 - val_loss: 1.8778 - learning_rate: 2.5000e-05\n",
            "Epoch 42/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 320ms/step - loss: 1.1909 - val_loss: 1.8921 - learning_rate: 2.5000e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.1598 - val_loss: 1.8977 - learning_rate: 2.5000e-05\n",
            "Epoch 44/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 2s/step - loss: 1.1440 - val_loss: 1.8746 - learning_rate: 2.5000e-05\n",
            "Epoch 45/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 2s/step - loss: 1.1474 - val_loss: 1.8470 - learning_rate: 2.5000e-05\n",
            "Epoch 46/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 1.1274 - val_loss: 1.8492 - learning_rate: 2.5000e-05\n",
            "Epoch 47/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.1170 - val_loss: 1.8747 - learning_rate: 2.5000e-05\n",
            "Epoch 48/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 314ms/step - loss: 1.1146 - val_loss: 1.8562 - learning_rate: 2.5000e-05\n",
            "Epoch 49/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 312ms/step - loss: 1.0721 - val_loss: 1.8744 - learning_rate: 2.5000e-05\n",
            "Epoch 50/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 2s/step - loss: 1.0729 - val_loss: 1.8310 - learning_rate: 2.5000e-05\n",
            "Epoch 51/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 1.0446 - val_loss: 1.8453 - learning_rate: 2.5000e-05\n",
            "Epoch 52/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 1.0428 - val_loss: 1.8426 - learning_rate: 2.5000e-05\n",
            "Epoch 53/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 314ms/step - loss: 1.0147 - val_loss: 1.8392 - learning_rate: 2.5000e-05\n",
            "Epoch 54/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 2s/step - loss: 1.0243 - val_loss: 1.8140 - learning_rate: 2.5000e-05\n",
            "Epoch 55/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 0.9762 - val_loss: 1.8262 - learning_rate: 2.5000e-05\n",
            "Epoch 56/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 2s/step - loss: 0.9991 - val_loss: 1.8047 - learning_rate: 2.5000e-05\n",
            "Epoch 57/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 320ms/step - loss: 0.9577 - val_loss: 1.8070 - learning_rate: 2.5000e-05\n",
            "Epoch 58/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 0.9293 - val_loss: 1.8198 - learning_rate: 2.5000e-05\n",
            "Epoch 59/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 2s/step - loss: 0.9158 - val_loss: 1.7989 - learning_rate: 2.5000e-05\n",
            "Epoch 60/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 2s/step - loss: 0.9054 - val_loss: 1.7486 - learning_rate: 2.5000e-05\n",
            "Epoch 61/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 321ms/step - loss: 0.8975 - val_loss: 1.8012 - learning_rate: 2.5000e-05\n",
            "Epoch 62/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 0.8688 - val_loss: 1.7613 - learning_rate: 2.5000e-05\n",
            "Epoch 63/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 2s/step - loss: 0.8828 - val_loss: 1.7310 - learning_rate: 2.5000e-05\n",
            "Epoch 64/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 0.8624 - val_loss: 1.7432 - learning_rate: 2.5000e-05\n",
            "Epoch 65/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 0.8482 - val_loss: 1.7656 - learning_rate: 2.5000e-05\n",
            "Epoch 66/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 313ms/step - loss: 0.8242 - val_loss: 1.7432 - learning_rate: 2.5000e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 312ms/step - loss: 0.8039 - val_loss: 1.7411 - learning_rate: 2.5000e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 2s/step - loss: 0.7944 - val_loss: 1.7198 - learning_rate: 2.5000e-05\n",
            "Epoch 69/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 0.7705 - val_loss: 1.7451 - learning_rate: 2.5000e-05\n",
            "Epoch 70/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 0.7845 - val_loss: 1.7625 - learning_rate: 2.5000e-05\n",
            "Epoch 71/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 2s/step - loss: 0.7507 - val_loss: 1.7121 - learning_rate: 2.5000e-05\n",
            "Epoch 72/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 3s/step - loss: 0.7335 - val_loss: 1.6953 - learning_rate: 2.5000e-05\n",
            "Epoch 73/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 2s/step - loss: 0.7423 - val_loss: 1.6874 - learning_rate: 2.5000e-05\n",
            "Epoch 74/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 0.6953 - val_loss: 1.7110 - learning_rate: 2.5000e-05\n",
            "Epoch 75/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 2s/step - loss: 0.6936 - val_loss: 1.6785 - learning_rate: 2.5000e-05\n",
            "Epoch 76/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 0.6817 - val_loss: 1.6882 - learning_rate: 2.5000e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 2s/step - loss: 0.6681 - val_loss: 1.6736 - learning_rate: 2.5000e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - loss: 0.6470 - val_loss: 1.6562 - learning_rate: 2.5000e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 318ms/step - loss: 0.6283 - val_loss: 1.6948 - learning_rate: 2.5000e-05\n",
            "Epoch 80/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - loss: 0.6253 - val_loss: 1.6683 - learning_rate: 2.5000e-05\n",
            "Epoch 81/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 314ms/step - loss: 0.5967 - val_loss: 1.6619 - learning_rate: 2.5000e-05\n",
            "Epoch 82/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - loss: 0.5999 - val_loss: 1.6513 - learning_rate: 2.5000e-05\n",
            "Epoch 83/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 319ms/step - loss: 0.5878 - val_loss: 1.6547 - learning_rate: 2.5000e-05\n",
            "Epoch 84/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 2s/step - loss: 0.5851 - val_loss: 1.6428 - learning_rate: 2.5000e-05\n",
            "Epoch 85/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 320ms/step - loss: 0.5687 - val_loss: 1.6477 - learning_rate: 2.5000e-05\n",
            "Epoch 86/100\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - loss: 0.5428"
          ]
        }
      ]
    }
  ]
}